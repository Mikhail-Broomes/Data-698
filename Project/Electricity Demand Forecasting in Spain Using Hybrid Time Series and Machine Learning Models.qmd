```{r}
# Libraries 

library(tidyverse)
library(magrittr)
library(lubridate)
library(tsibble)
library(imputeTS)
library(forecast)
library(prophet)
library(tseries)
library(caret)
library(xgboost)
library(randomForest)
library(keras)
library(tensorflow)
library(fable)
library(feasts)
library(glue)
library(solitude)
library(knitr)
library(kableExtra)
library(timetk)
library(lightgbm)
library(torch)
library(tidyr)
library(lmtest)
library(Metrics)
library(iml)
library(ggcorrplot)
library(tinytex)
library(zoo)
library(MASS)
library(readr)
library(dplyr)
library(ggplot2)


# Setting seed for reproducibility
set.seed(592)
```

---
title: "Electricity Demand Forecasting in Spain Using Hybrid Time Series and Machine Learning Models"
author: "Mikhail Broomes"
format: html
editor: visual
---

## Abstract 

This study evaluates short-term electricity demand forecasting in Spain by leveraging classical statistical models, machine learning algorithms, and hybrid approaches. Core models include SARIMAX, XGBoost, and an ARIMA-ANN hybrid. Additional benchmarks such as TBATS, NNETAR, and STL-ARIMA are explored. The methodology incorporates extensive feature engineering, SHAP-based interpretability, and rolling-origin cross-validation. Results demonstrate that the tuned XGBoost model significantly outperforms classical statistical models in RMSE and MAPE, with hybrid models offering strong complementary performance.

# 1. Introduction 

#### 1.1 Motivation

Spain’s electricity grid, heavily reliant on renewables, presents high volatility. The nationwide blackout in March 2025 highlighted vulnerabilities in existing short-term forecasting systems.

#### 1.2 Objectives 

The objectives of this study are to

\(1\) compare classical, machine learning, and hybrid models,

\(2\) focus on 24–168 hour short-term demand forecasting

\(3\) apply SHAP for explainability, scenario testing, and rolling-origin cross-validation.

# 2. Literature Review

Prior studies have explored time series models such as SARIMA and TBATS for electricity load forecasting (Hyndman, 2018). Zhang (2003) proposed hybrid ARIMA-ANN models to capture nonlinear residual behavior, offering a powerful alternative when classical models fall short. In the context of machine learning, Li et al. (2020) and Lundberg & Lee (2017) demonstrated the effectiveness of XGBoost and SHAP for improving both forecasting accuracy and interpretability. For model evaluation, rolling-origin cross-validation and the Diebold-Mariano test (Diebold & Mariano, 1995) have become standard practices in comparing time series prediction performance.

# 3. Data & Preprocessing 

#### 3.1 Data Sources:

The dataset used in this study combines electricity load and weather data. The electricity load data was sourced from ENTSO-E and includes hourly records spanning from 2015 to 2019. Complementary meteorological variables, including temperature, humidity, wind speed, and solar radiation, were obtained from publicly available weather sources. These variables are known to have a significant influence on electricity demand.

#### 3.2 Pre-processing and Feature Engineering

```{r}

data <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")
#data <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
# Convert time to POSIXct
data$time <- as.POSIXct(data$time, format = "%m/%d/%y %H:%M", tz = "UTC")

# Convert to tsibble
data_ts <- tsibble::as_tsibble(data, index = time)

# Add temporal features
data_ts <- data_ts %>%
  mutate(
    hour = hour(time),
    wday = wday(time, label = TRUE),
    month = month(time, label = TRUE)
  )

# Check if data is sorted by time
data_ts <- data_ts %>% arrange(time)

load_ts <- ts(data_ts$total.load.actual, frequency = 24)

```

To prepare the data for modeling, missing values were imputed using time-series-specific methods such as linear interpolation and Kalman smoothing. Several temporal features were engineered to capture cyclical demand patterns. These included hour of day, day of the week, and month of the year.

Lagged values and rolling means of the target variable were created to help the models learn recent trends. In addition, some variables were normalized or transformed to stabilize variance and remove skewness. The resulting dataset was structured as a multivariate time series with over 35,000 observations and multiple engineered predictors. (2015–2019)

#### 3.3 Feature Engineering

The following code implements three core types of feature engineering to enhance model performance:

1.  **Lag features** — These features capture recent load behavior by shifting historical load values. For example, the 1-hour, 24-hour (daily), and 168-hour (weekly) lag features help models learn from short- and medium-term temporal dependencies.

2.  **Rolling averages** — These smooth out short-term fluctuations and help the model detect broader trends. Here, 3-hour and 24-hour rolling means are calculated.

3.  **Multi-seasonal decomposition** — The `mstl()` function decomposes the load series into seasonal, trend, and remainder components across multiple periodicities (daily and weekly), which is helpful for understanding the cyclical nature of the data and guiding model structure.

```{r}
# Create lag features
data_ts <- data_ts %>%
  arrange(time) %>%
  mutate(
    load_lag_1h = lag(total.load.actual, 1),
    load_lag_24h = lag(total.load.actual, 24),
    load_lag_168h = lag(total.load.actual, 168)  # weekly lag
  )

# Create rolling averages
library(zoo)
data_ts$roll_mean_3h <- rollmean(data_ts$total.load.actual, k = 3, fill = NA, align = "right")
data_ts$roll_mean_24h <- rollmean(data_ts$total.load.actual, k = 24, fill = NA, align = "right")

# Perform time-based decomposition
library(forecast)

# Multi-seasonal decomposition with both daily and weekly patterns
ts_multi <- msts(data_ts$total.load.actual, seasonal.periods = c(24, 168))

decomp_mstl <- mstl(ts_multi)
autoplot(decomp_mstl)
```

This section outlines a broad feature engineering plan grouped into four categories. While only a subset of these features is currently implemented, the dataset contains the necessary raw inputs to support further development. However, the dataset includes many raw variables that provide a strong foundation for extending engineered features in future work.

**Temporal Features**

-   Implemented: Extracted hour, weekday, and month using `lubridate`

-   Not implemented: Cyclical encoding (sine/cosine), public holiday flags, or interaction terms

-   Available: Timestamps suitable for deriving advanced temporal patterns

**Weather Features**

-   Implemented: Included raw temperature, humidity, wind speed, pressure, and cloud cover

-   Not implemented: Moving averages, temperature-humidity interaction terms, or degree day indicators

-   Available: Detailed weather metrics at hourly resolution suitable for expansion

**Economic Features**

-   Not implemented: No economic indices or price volatility metrics included in the current dataset

**Renewable Energy Features**

-   Implemented: Hourly solar and wind generation, forecast values

-   Not implemented: Forecast error metrics, ramp rates, renewable penetration ratios

-   Available: Sufficient generation and forecast variables to compute volatility and ramp behavior

Future development could expand the current feature set using transformations such as lagged economic indicators, weather-derived metrics, and renewable volatility measures. (2015–2019)

```{r}
library(ggplot2)

# Create correlation plot manually
lags_df <- data.frame(
  Lag = c(1, 24, 168),
  Correlation = c(
    cor(data_ts$total.load.actual, lag(data_ts$total.load.actual, 1), use = "complete.obs"),
    cor(data_ts$total.load.actual, lag(data_ts$total.load.actual, 24), use = "complete.obs"),
    cor(data_ts$total.load.actual, lag(data_ts$total.load.actual, 168), use = "complete.obs")
  )
)

ggplot(lags_df, aes(x = factor(Lag), y = Correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Correlation with Lagged Load Values", x = "Lag (hours)", y = "Correlation") +
  theme_minimal()

```

# 4. Exploratory Data Analysis

This section provides a comprehensive examination of the temporal and seasonal structure of electricity load using visual and statistical tools. The analyses confirm strong periodic patterns that justify the use of time series models with seasonal components.

### STL Decomposition of Electricity Load

```{r}
# STL decomposition

stl_decomp <- data_ts %>%
  model(STL(total.load.actual ~ season(window = "periodic"))) %>%
  components()

autoplot(stl_decomp) +
  labs(title = "STL Decomposition of Electricity Load")
```

This STL decomposition illustrates the long-term trend, along with yearly and weekly seasonal components. It confirms strong periodic structure in the electricity load data.

### ACF and PACF Plots

```{r}
data_ts %>% 
  ACF(total.load.actual, lag_max = 168) %>%
  autoplot() +
  labs(title = "ACF of Electricity Load")

data_ts %>%
  PACF(total.load.actual, lag_max = 168) %>%
  autoplot() +
  labs(title = "PACF of Electricity Load")
```

The ACF plot shows clear spikes at 24 and 168 lags, confirming daily and weekly seasonality. The PACF plot suggests strong lag-1 dependency, which supports short-term autoregressive modeling.

# 5. Model Implementation

## 5.1 Primary Models

This study implements and compares three primary modeling frameworks for short-term electricity demand forecasting:

-   **SARIMAX**: A classical time series model that incorporates exogenous regressors (e.g., temperature, humidity) to capture both temporal autocorrelation and external influences.

-   **XGBoost**: A powerful tree-based machine learning algorithm, known for its ability to capture nonlinear patterns. The model is tuned using cross-validation and includes engineered lag and rolling features.

-   **ARIMA-ANN Hybrid**: This model combines a linear ARIMA model with a nonlinear neural network applied to the residuals. Implemented using the `ARIMAANN` package, this hybrid approach addresses both trend and nonlinear structure in electricity demand.

```{r}
library(dplyr)
library(tsibble)
library(lubridate)
library(zoo)

# Read the data
#data <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
data <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")

# Convert time to POSIXct
data$time <- as.POSIXct(data$time, format = "%m/%d/%y %H:%M", tz = "UTC")

# Convert to tsibble
data_ts <- tsibble::as_tsibble(data, index = time)

# Add temporal features
data_ts <- data_ts %>%
  mutate(
    hour = hour(time),
    weekday = wday(time, label = TRUE),
    month = month(time, label = TRUE)
  )

# Check if data is sorted by time
data_ts <- data_ts %>% arrange(time)

# Drop columns that are all NA (prevents na.omit from removing all rows)
na_cols <- sapply(data_ts, function(x) all(is.na(x)))
data_ts <- data_ts[, !na_cols]

# Unified train-test split with proper feature engineering
library(dplyr)
library(zoo)

# Add lag and rolling features before removing NAs
data_ts <- data_ts %>%
  arrange(time) %>%
  mutate(
    load_lag_1h = lag(total.load.actual, 1),
    load_lag_24h = lag(total.load.actual, 24),
    load_lag_168h = lag(total.load.actual, 168),
    roll_mean_3h = rollmean(total.load.actual, k = 3, fill = NA, align = "right")
  )

# Remove rows with NA after feature creation
data_model <- na.omit(data_ts)

# Time series object
load_ts <- ts(data_model$total.load.actual, frequency = 24)
n <- length(load_ts)
h <- 168  # One-week forecast horizon

# Train/test split
train_ts <- head(load_ts, n - h)
test_ts  <- tail(load_ts, h)

# Feature selection for machine learning models
features <- c("temp", "humidity", "hour", "weekday", "load_lag_1h", "load_lag_24h", "roll_mean_3h")
x_train <- as.matrix(data_model[1:(n - h), features])
x_test  <- as.matrix(data_model[(n - h + 1):n, features])
y_train <- data_model$total.load.actual[1:(n - h)]
y_test  <- data_model$total.load.actual[(n - h + 1):n]



```

### 5.1.1 SARIMAX

SARIMAX is a classical time series model that includes autoregressive, differencing, and moving average components, while also incorporating external regressors like temperature and humidity. This makes it especially useful for capturing lag-based relationships and exogenous influences in electricity demand.

In this study, variable selection for SARIMAX was based on stepwise regression using the Akaike Information Criterion (AIC), a statistically supported method that balances model fit and complexity. Starting from the full set of candidate regressors, the stepwise AIC approach identified 25 variables that significantly contributed to explaining electricity load. These included weather features (e.g., `temp`, `humidity`, `wind_speed`), generation metrics (e.g., `generation.wind.onshore`, `generation.nuclear`), and economic indicators (e.g., `price.day.ahead`).

```{r}
# Load and clean data
#df <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
df <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")
df$time <- as.POSIXct(df$time, format = "%m/%d/%y %H:%M", tz = "UTC")
df <- df[order(df$time), ]

# Drop irrelevant or low-quality columns
drop_cols <- c(
  "time", "time_hour", "total.load.forecast", "price.actual",
  "generation.hydro.pumped.storage.aggregated",
  "forecast.wind.offshore.day.ahead", "forecast.solar.day.ahead",
  "forecast.wind.onshore.day.ahead"
)
df_model <- df[, !(names(df) %in% drop_cols)]

# Drop columns that are all NA or nearly all NA
na_cols <- sapply(df_model, function(x) all(is.na(x)) || sum(is.na(x)) > 0.25 * length(x))
df_model <- df_model[, !na_cols]

# Drop rows with NA values
df_model <- na.omit(df_model)

# Check again before modeling
if (nrow(df_model) == 0) {
  stop("No non-NA rows remain after filtering.")
}

# Stepwise regression for variable selection
lm_model <- lm(total.load.actual ~ ., data = df_model)
step_model <- stepAIC(lm_model, direction = "both", trace = FALSE)
best_vars <- names(coef(step_model))[-1]   

```

```{r}
xreg_train <- as.matrix(data_model[1:(n - h), best_vars])
xreg_test <- as.matrix(data_model[(n - h + 1):n, best_vars])

sarimax_model <- Arima(train_ts,
                       order = c(5, 1, 0),
                       seasonal = list(order = c(2, 0, 0), period = 24),
                       xreg = xreg_train)

# Forecasting
fc_sarimax <- forecast(sarimax_model, xreg = xreg_test, h = h)
predicted <- as.numeric(fc_sarimax$mean)
actual <- as.numeric(test_ts)

# Evaluation
rmse <- sqrt(mean((predicted - actual)^2))
mae <- mean(abs(predicted - actual))
mape <- mean(abs((predicted - actual) / actual)) * 100

cat("
SARIMAX Accuracy:
")
cat("MAE :", round(mae, 2), "
")
cat("RMSE:", round(rmse, 2), "
")
cat("MAPE:", round(mape, 2), "%
")
```

**SARIMAX Model Results**

```{r}
# SARIMAX Forecast Plot
library(ggplot2)

results_df <- data.frame(
  Time = data_model$time[(n - h + 1):n],
  Actual = actual,
  Forecast = predicted
)

ggplot(results_df, aes(x = Time)) +
  geom_line(aes(y = Actual, color = "Actual")) +
  geom_line(aes(y = Forecast, color = "Forecast")) +
  labs(title = "SARIMAX Forecast vs Actual",
       x = "Time",
       y = "Load (MW)",
       color = "Legend") +
  theme_minimal()
```

The SARIMAX model produced strong forecasting accuracy, with an MAE of **2954.99**, RMSE of **3443.95**, and MAPE of **10.81%**. These results reflect the model’s ability to capture both seasonal structure and influence of external regressors like temperature, solar generation, and market prices. The relatively low MAPE demonstrates its suitability for short-term electricity forecasting in a volatile, renewable-integrated grid environment.

### 5.1.2 XGBoost 

XGBoost is a gradient-boosted decision tree algorithm optimized for predictive performance. It is trained on engineered features including lags, rolling means, and calendar variables. The model is tuned using time-series-aware cross-validation and excels at capturing nonlinear dependencies that traditional linear models may miss.

```{r}
# Install additional packages if needed
# if (!require(SHAP)) install.packages("SHAP")
# library(SHAP)

# Set seed for reproducibility
set.seed(592)

# Step 1: Load and parse data
#xgb_df <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
xgb_df <-read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")

xgb_df$time <- as.POSIXct(xgb_df$time, format = "%m/%d/%y %H:%M", tz = "UTC")

# Step 2: Enhanced feature engineering with proper lag handling
xgb_df <- xgb_df %>%
  arrange(time) %>%
  mutate(
    # Time-based features with cyclical encoding
    hour = hour(time),
    wday = wday(time),
    month = month(time),
    # Cyclical encodings for smoother seasonality
    hour_sin = sin(2 * pi * hour / 24),
    hour_cos = cos(2 * pi * hour / 24),
    month_sin = sin(2 * pi * month / 12),
    month_cos = cos(2 * pi * month / 12),
    wday_sin = sin(2 * pi * wday / 7),
    wday_cos = cos(2 * pi * wday / 7),
    # Proper lag features (past values only)
    load_lag_1h = lag(total.load.actual, 1),
    load_lag_24h = lag(total.load.actual, 24),
    load_lag_168h = lag(total.load.actual, 168),
    # Rolling means with proper alignment (right-aligned, past values only)
    roll_mean_3h = zoo::rollmeanr(total.load.actual, 3, fill = NA),
    roll_mean_24h = zoo::rollmeanr(total.load.actual, 24, fill = NA),
    roll_mean_168h = zoo::rollmeanr(total.load.actual, 168, fill = NA),
    # Additional statistical features
    roll_sd_24h = zoo::rollapplyr(total.load.actual, 24, sd, fill = NA),
    roll_max_24h = zoo::rollapplyr(total.load.actual, 24, max, fill = NA),
    roll_min_24h = zoo::rollapplyr(total.load.actual, 24, min, fill = NA),
    # Trend features
    load_diff_1h = total.load.actual - lag(total.load.actual, 1),
    load_diff_24h = total.load.actual - lag(total.load.actual, 24)
  )

# Step 3: Select features
feature_cols <- c(
  "temp", "humidity", "pressure",
  "generation.solar", "generation.wind.onshore",
  "hour_sin", "hour_cos", "month_sin", "month_cos", "wday_sin", "wday_cos",
  "load_lag_1h", "load_lag_24h", "load_lag_168h",
  "roll_mean_3h", "roll_mean_24h", "roll_mean_168h",
  "roll_sd_24h", "roll_max_24h", "roll_min_24h",
  "load_diff_1h", "load_diff_24h"
)

# Keep original time-based features for comparison
original_time_features <- c("hour", "wday", "month")
all_features <- c(feature_cols, original_time_features)

xgb_df <- xgb_df[, c("total.load.actual", "time", all_features)]
xgb_df <- xgb_df[complete.cases(xgb_df), ]

# Step 4: Enhanced correlation filtering
cor_matrix <- cor(xgb_df[, feature_cols], use = "complete.obs")
high_corr_vars <- c()
threshold <- 0.9

# Find highly correlated features
for (i in 1:(ncol(cor_matrix) - 1)) {
  for (j in (i + 1):ncol(cor_matrix)) {
    if (abs(cor_matrix[i, j]) > threshold) {
      # Keep the feature with higher correlation to target
      target_cor_i <- abs(cor(xgb_df[[rownames(cor_matrix)[i]]], xgb_df$total.load.actual))
      target_cor_j <- abs(cor(xgb_df[[colnames(cor_matrix)[j]]], xgb_df$total.load.actual))
      
      if (target_cor_i > target_cor_j) {
        var_to_remove <- colnames(cor_matrix)[j]
      } else {
        var_to_remove <- rownames(cor_matrix)[i]
      }
      
      if (!(var_to_remove %in% high_corr_vars)) {
        high_corr_vars <- c(high_corr_vars, var_to_remove)
      }
    }
  }
}

selected_features <- setdiff(feature_cols, high_corr_vars)
cat("Removed due to high correlation:", paste(high_corr_vars, collapse = ", "), "\n")
cat("Final selected features:", paste(selected_features, collapse = ", "), "\n")

# Step 5: Prepare data with optional scaling
X <- as.matrix(xgb_df[, selected_features])
y <- xgb_df$total.load.actual
times <- xgb_df$time

# Optional: Feature scaling (comment out if not desired)
# preProcValues <- preProcess(X, method = c("center", "scale"))
# X <- predict(preProcValues, X)

# Step 6: Time series train/test split
split_point <- floor(0.8 * nrow(X))
train_x <- X[1:split_point, ]
train_y <- y[1:split_point]
test_x <- X[(split_point + 1):nrow(X), ]
test_y <- y[(split_point + 1):length(y)]
train_times <- times[1:split_point]
test_times <- times[(split_point + 1):length(times)]

# Step 7: Hyperparameter tuning with XGBoost's built-in CV
# Prepare DMatrix
dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)

# Define parameter grid for manual tuning
param_grid <- list(
  list(eta = 0.05, max_depth = 3),
  list(eta = 0.05, max_depth = 6),
  list(eta = 0.05, max_depth = 9),
  list(eta = 0.1, max_depth = 3),
  list(eta = 0.1, max_depth = 6),
  list(eta = 0.1, max_depth = 9),
  list(eta = 0.2, max_depth = 3),
  list(eta = 0.2, max_depth = 6),
  list(eta = 0.2, max_depth = 9)
)

# Function to perform time series cross-validation
time_series_cv <- function(params, nrounds = 200) {
  # Create time-based folds
  n_train <- nrow(train_x)
  n_folds <- 5
  fold_size <- floor(n_train / (n_folds + 1))
  
  cv_scores <- c()
  
  for (i in 1:n_folds) {
    # Create train and validation sets for this fold
    train_end <- fold_size * (i + 1)
    val_start <- train_end + 1
    val_end <- min(train_end + fold_size, n_train)
    
    if (val_start <= n_train) {
      fold_train_x <- train_x[1:train_end, ]
      fold_train_y <- train_y[1:train_end]
      fold_val_x <- train_x[val_start:val_end, ]
      fold_val_y <- train_y[val_start:val_end]
      
      fold_dtrain <- xgb.DMatrix(data = fold_train_x, label = fold_train_y)
      fold_dval <- xgb.DMatrix(data = fold_val_x, label = fold_val_y)
      
      # Train model
      fold_model <- xgb.train(
        params = c(list(objective = "reg:squarederror"), params),
        data = fold_dtrain,
        nrounds = nrounds,
        verbose = 0
      )
      
      # Predict and calculate RMSE
      fold_preds <- predict(fold_model, fold_dval)
      fold_rmse <- sqrt(mean((fold_preds - fold_val_y)^2))
      cv_scores <- c(cv_scores, fold_rmse)
    }
  }
  
  return(mean(cv_scores))
}

# Tune hyperparameters
best_rmse <- Inf
best_params <- NULL
best_nrounds <- NULL

cat("Tuning hyperparameters...\n")
for (params in param_grid) {
  # Find optimal number of rounds
  cv_result <- xgb.cv(
    params = c(list(objective = "reg:squarederror"), params),
    data = dtrain,
    nrounds = 300,
    nfold = 5,
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  optimal_nrounds <- cv_result$best_iteration
  
  # Evaluate with time series CV
  cv_rmse <- time_series_cv(params, optimal_nrounds)
  
  cat(sprintf("eta: %.2f, max_depth: %d, nrounds: %d, CV RMSE: %.4f\n", 
              params$eta, params$max_depth, optimal_nrounds, cv_rmse))
  
  if (cv_rmse < best_rmse) {
    best_rmse <- cv_rmse
    best_params <- params
    best_nrounds <- optimal_nrounds
  }
}

cat("\nBest parameters:\n")
cat(sprintf("eta: %.2f, max_depth: %d, nrounds: %d, CV RMSE: %.4f\n", 
            best_params$eta, best_params$max_depth, best_nrounds, best_rmse))

# Train final model with best parameters
final_params <- c(list(objective = "reg:squarederror"), best_params)
xgb_model <- xgb.train(
  params = final_params,
  data = dtrain,
  nrounds = best_nrounds,
  verbose = 0
)
```

XGBoost Model Results


The XGBoost model used a correlation-filtered set of engineered features, with highly collinear variables such as `load_lag_1h` and `roll_mean_24h` removed. The final selected variables included predictors such as temperature, humidity, solar and wind generation, lagged demand values, and trend differentials.

Hyperparameter tuning was performed across 9 configurations using a combination of XGBoost’s built-in `xgb.cv()` and a custom time-series cross-validation function. The best-performing configuration was:

-   **eta**: 0.05

-   **max_depth**: 9

-   **nrounds**: 300

-   **CV RMSE**: 157.02

These results demonstrate that XGBoost is capable of modeling complex, nonlinear relationships in load demand. Its performance improvement over classical models validates the benefit of engineered lag, trend, and weather features in short-term electricity forecasting.

```{r}

# Load and preprocess data
#xgb_df <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
xgb_df <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")
xgb_df$time <- as.POSIXct(xgb_df$time, format = "%m/%d/%y %H:%M", tz = "UTC")

# Feature engineering
xgb_df <- xgb_df %>%
  arrange(time) %>%
  mutate(
    hour = hour(time),
    wday = wday(time),
    month = month(time),
    hour_sin = sin(2 * pi * hour / 24),
    hour_cos = cos(2 * pi * hour / 24),
    month_sin = sin(2 * pi * month / 12),
    month_cos = cos(2 * pi * month / 12),
    wday_sin = sin(2 * pi * wday / 7),
    wday_cos = cos(2 * pi * wday / 7),
    load_lag_1h = lag(total.load.actual, 1),
    load_lag_24h = lag(total.load.actual, 24),
    load_lag_168h = lag(total.load.actual, 168),
    roll_mean_3h = rollmeanr(total.load.actual, 3, fill = NA),
    roll_mean_24h = rollmeanr(total.load.actual, 24, fill = NA),
    roll_mean_168h = rollmeanr(total.load.actual, 168, fill = NA),
    roll_sd_24h = rollapplyr(total.load.actual, 24, sd, fill = NA),
    roll_max_24h = rollapplyr(total.load.actual, 24, max, fill = NA),
    roll_min_24h = rollapplyr(total.load.actual, 24, min, fill = NA),
    load_diff_1h = total.load.actual - lag(total.load.actual, 1),
    load_diff_24h = total.load.actual - lag(total.load.actual, 24)
  )

#Select features
feature_cols <- c("temp", "humidity", "pressure", "generation.solar", "generation.wind.onshore",
                  "hour_sin", "hour_cos", "month_sin", "month_cos", "wday_sin", "wday_cos",
                  "load_lag_1h", "load_lag_24h", "load_lag_168h", "roll_mean_3h", "roll_mean_24h",
                  "roll_mean_168h", "roll_sd_24h", "roll_max_24h", "roll_min_24h",
                  "load_diff_1h", "load_diff_24h")
original_time_features <- c("hour", "wday", "month")
all_features <- c(feature_cols, original_time_features)

xgb_df <- xgb_df[, c("total.load.actual", "time", all_features)]
xgb_df <- xgb_df[complete.cases(xgb_df), ]

# Correlation filtering
cor_matrix <- cor(xgb_df[, feature_cols])
high_corr_vars <- c()
threshold <- 0.9
for (i in 1:(ncol(cor_matrix) - 1)) {
  for (j in (i + 1):ncol(cor_matrix)) {
    if (abs(cor_matrix[i, j]) > threshold) {
      target_cor_i <- abs(cor(xgb_df[[rownames(cor_matrix)[i]]], xgb_df$total.load.actual))
      target_cor_j <- abs(cor(xgb_df[[colnames(cor_matrix)[j]]], xgb_df$total.load.actual))
      var_to_remove <- ifelse(target_cor_i > target_cor_j, colnames(cor_matrix)[j], rownames(cor_matrix)[i])
      if (!(var_to_remove %in% high_corr_vars)) {
        high_corr_vars <- c(high_corr_vars, var_to_remove)
      }
    }
  }
}
selected_features <- setdiff(feature_cols, high_corr_vars)

#Prepare matrices
X <- as.matrix(xgb_df[, selected_features])
y <- xgb_df$total.load.actual
times <- xgb_df$time

#Train-test split
split_point <- floor(0.8 * nrow(X))
train_x <- X[1:split_point, ]
train_y <- y[1:split_point]
test_x <- X[(split_point + 1):nrow(X), ]
test_y <- y[(split_point + 1):length(y)]
train_times <- times[1:split_point]
test_times <- times[(split_point + 1):length(times)]

# Final training and evaluation
final_params <- list(objective = "reg:squarederror", eta = 0.05, max_depth = 9)
dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)
xgb_model <- xgb.train(params = final_params, data = dtrain, nrounds = 300, verbose = 0)
xgb_preds <- predict(xgb_model, newdata = dtest)

# Manual evaluation
rmse_xgb <- sqrt(mean((xgb_preds - test_y)^2))
mae_xgb <- mean(abs(xgb_preds - test_y))
mape_xgb <- mean(abs((xgb_preds - test_y) / test_y)) * 100
cat("\nManual XGBoost Evaluation:\n")
cat("MAE :", round(mae_xgb, 2), "\n")
cat("RMSE:", round(rmse_xgb, 2), "\n")
cat("MAPE:", round(mape_xgb, 2), "%\n")

# Plot forecast
xgb_results <- data.frame(Time = test_times, Actual = test_y, Forecast = xgb_preds)
ggplot(xgb_results, aes(x = Time)) +
  geom_line(aes(y = Actual, color = "Actual")) +
  geom_line(aes(y = Forecast, color = "Forecast")) +
  labs(title = "XGBoost Forecast vs Actual", x = "Time", y = "Electricity Load (MW)", color = "Legend") +
  theme_minimal()
```

```{r}
# Filter for a 1-week window (e.g., last 168 hours)
plot_data <- tail(xgb_results, 168)

# Plot: Forecast vs Actual (Zoomed)
library(ggplot2)
ggplot(plot_data, aes(x = Time)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 0.6) +
  geom_line(aes(y = Forecast, color = "Forecast"), linewidth = 0.6) +
  scale_color_manual(values = c("Actual" = "red", "Forecast" = "cyan4")) +
  labs(
    title = "XGBoost Forecast vs Actual (Zoomed)",
    y = "Electricity Load (MW)",
    x = "Time",
    color = "Legend"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

```

### 5.1.3 ARIMA-ANN

```{r}
library(ARIMAANN)
library(forecast)

# Load data and format time series
library(tsibble)
#data <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
data <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")
data$time <- as.POSIXct(data$time, format = "%m/%d/%y %H:%M", tz = "UTC")
data_ts <- as_tsibble(data, index = time)
data_ts <- data_ts %>% arrange(time)
load_ts <- ts(data_ts$total.load.actual, frequency = 24)

# Train/test split
n <- length(load_ts)
h <- 168
train_ts <- head(load_ts, n - h)
test_ts  <- tail(load_ts, h)


```

```{r}
# Use last 500 points only
train_subset <- tail(train_ts, 500)

# Fit ARIMA-ANN
fit_arima_ann <- ARIMAANN(train_subset, h = h)

#fit_arima_ann <- ARIMAANN(train_ts, h = h)


# Forecast
predicted <- as.numeric(fit_arima_ann$forecast)
actual <- as.numeric(test_ts)

# Evaluation
rmse <- sqrt(mean((predicted - actual)^2))
mae <- mean(abs(predicted - actual))
mape <- mean(abs((predicted - actual) / actual)) * 100

cat("
ARIMA-ANN Hybrid Accuracy:
")
cat("MAE :", round(mae, 2), "
")
cat("RMSE:", round(rmse, 2), "
")
cat("MAPE:", round(mape, 2), "%
")
```

```{r}
# Plot forecast vs actual (Zoomed 1-week window)
plot_data <- data.frame(
  Time = tail(test_times, h),
  Actual = tail(actual, h),
  Forecast = tail(predicted, h)
)

ggplot(plot_data, aes(x = Time)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 0.6) +
  geom_line(aes(y = Forecast, color = "Forecast"), linewidth = 0.6) +
  scale_color_manual(values = c("Actual" = "red", "Forecast" = "cyan4")) +
  labs(
    title = "ARIMA-ANN Forecast vs Actual (Zoomed)",
    y = "Electricity Load (MW)",
    x = "Time",
    color = "Legend"
  ) +
  theme_minimal() +
  theme(legend.position = "right")
```



## 5.2 Secondary Baselines

### 5.2.1 TBATS 

```{r}
 # Read the dataset
#data <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
    data <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")
    #data <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
    # Convert time to POSIXct
    data$time <- as.POSIXct(data$time, format = "%m/%d/%y %H:%M", tz = "UTC")

    # Convert to tsibble
    data_ts <- tsibble::as_tsibble(data, index = time)

    # Add temporal features
    data_ts <- data_ts %>%
      mutate(
        hour = hour(time),
        wday = wday(time, label = TRUE),
        month = month(time, label = TRUE)
      )

    # Check if data is sorted by time
    data_ts <- data_ts %>% arrange(time)

    # Create a univariate time series from the tsibble data
    # Important: use data_ts (the tsibble) not data (the original data frame)
    load_ts <- ts(data_ts$total.load.actual, frequency = 24)

    # Check data
    cat("Total observations:", length(load_ts), "\n")
    cat("First few values:", head(load_ts), "\n")
    cat("Last few values:", tail(load_ts), "\n")

    # Hold out last 168 hours (1 week)
    h <- 168
    n <- length(load_ts)

    # Ensure we have enough data
    if (n <= h) {
      stop("Not enough data: need at least ", h + 1, " observations, but only have ", n)
    }

    # Split the data
    train_ts <- ts(head(load_ts, n - h), frequency = 24)
    test_ts  <- ts(tail(load_ts, h), frequency = 24)

 
```

```{r}
  # Fit TBATS model
    cat("Fitting TBATS model...\n")
    model_tbats <- tbats(train_ts)

    # Generate forecasts
    cat("Generating forecasts...\n")
    fc_tbats <- forecast(model_tbats, h = h)

    # Extract predicted and actual values
    predicted <- as.numeric(fc_tbats$mean)
    actual    <- as.numeric(test_ts)

    cat("Length of predicted:", length(predicted), "\n")
    cat("Length of actual:", length(actual), "\n")

    # Check for any issues
    if (length(predicted) != length(actual)) {
      cat("ERROR: Length mismatch between predicted and actual values!\n")
      cat("Predicted length:", length(predicted), "\n")
      cat("Actual length:", length(actual), "\n")
    } else if (anyNA(predicted)) {
      cat("ERROR: NA values found in predictions!\n")
      cat("Number of NAs in predicted:", sum(is.na(predicted)), "\n")
    } else if (anyNA(actual)) {
      cat("ERROR: NA values found in actual data!\n")
      cat("Number of NAs in actual:", sum(is.na(actual)), "\n")
    } else {
      # Calculate accuracy metrics
      acc_tbats <- accuracy(predicted, actual)
      print(acc_tbats)
      
      # Additional diagnostics
      cat("\nModel summary:\n")
      print(model_tbats)
    }

    # Optional: Visualize the forecast
    if (!anyNA(predicted) && !anyNA(actual)) {
      # Create a simple plot
      plot(fc_tbats, main = "TBATS Forecast vs Actual")
      lines(actual, col = "red", lwd = 2)
      legend("topright", c("Forecast", "Actual"), col = c("blue", "red"), lty = 1)
    }
```

The TBATS model, known for handling multiple and complex seasonalities, was applied to the hourly electricity load data using a 7-day forecasting window. The model automatically selected a Box-Cox transformation with λ = 0.07 and identified two seasonal periods corresponding to daily (24 hours) and weekly (168 hours) patterns. The model's damping parameter was 0.941, indicating a smoothed trend component.

Despite the absence of ARMA terms, the TBATS model achieved a good fit, evidenced by a low residual standard deviation (σ = 0.053) and a reasonable AIC value of 825,376. The forecast captured the daily cyclical load variation and aligned well with observed values over the 168-hour test window, as visualized in the forecast plot. This performance highlights TBATS as a strong candidate for short-term electricity demand forecasting when seasonality is a dominant driver.

```{r}
# More detailed accuracy information
cat("\nDetailed Accuracy Metrics:\n")
print(acc_tbats)

# Check the structure of the accuracy object
cat("\nAccuracy object structure:\n")
str(acc_tbats)

# Calculate individual metrics manually if needed
mae <- mean(abs(predicted - actual))
rmse <- sqrt(mean((predicted - actual)^2))
mape <- mean(abs((predicted - actual)/actual)) * 100

cat("\nManual calculations:\n")
cat("MAE:", mae, "\n")
cat("RMSE:", rmse, "\n")
cat("MAPE:", mape, "%\n")

# Create a comparison plot
library(ggplot2)
library(dplyr)

# Create data frame for plotting
results_df <- data.frame(
  time = 1:168,
  predicted = predicted,
  actual = actual
)

# Plot comparison
ggplot(results_df, aes(x = time)) +
  geom_line(aes(y = actual, color = "Actual"), size = 1) +
  geom_line(aes(y = predicted, color = "Predicted"), size = 1) +
  labs(title = "TBATS Forecast vs Actual",
       x = "Hours Ahead",
       y = "Load (MW)",
       color = "Type") +
  theme_minimal()
```

This tells us the model components:

-    `0.07`: Box-Cox transformation λ (closer to 0 = log-like)

-    `{0,0}`: No ARMA errors used (p = 0, q = 0)

-    `0.941`: Damping parameter for trend smoothing

-    `{<24,7>}`: Dual seasonality — **daily (24)** and **weekly (168/24 = 7)**

## 5.2.2 NNETAR (simple, medium, complex)

```{r}
  library(forecast)
    library(Metrics)

    # Reuse your existing test set
    actual <- as.numeric(test_ts)

    # Define NNETAR configurations to test
    configs <- list(
      simple = list(size = 3, p = 3, P = 1),
      medium = list(size = 5, p = 7, P = 2),
      complex = list(size = 7, p = 14, P = 2)
    )

    # Initialize results list
    results <- list()

    # Try each configuration
    for (name in names(configs)) {
      cat("\n--- Trying NNETAR configuration:", name, "---\n")
      cfg <- configs[[name]]
      
      tryCatch({
        model <- nnetar(train_ts, 
                        size = cfg$size, 
                        p = cfg$p, 
                        P = cfg$P, 
                        repeats = 10)

        fc <- forecast(model, h = length(actual))
        predicted <- as.numeric(fc$mean)

        if (length(predicted) == length(actual) && !anyNA(predicted) && !anyNA(actual)) {
          mae <- mae(actual, predicted)
          rmse <- rmse(actual, predicted)
          mape <- mape(actual, predicted) * 100

          cat("MAE :", round(mae, 2), "\n")
          cat("RMSE:", round(rmse, 2), "\n")
          cat("MAPE:", round(mape, 2), "%\n")

          results[[name]] <- list(MAE = mae, RMSE = rmse, MAPE = mape)
        } else {
          cat("⚠️ Skipped due to NA or length mismatch\n")
        }
      }, error = function(e) {
        cat("❌ Error fitting model:", e$message, "\n")
      })
    }

    # Print summary of results
    cat("\n=== NNETAR Configuration Summary ===\n")
    for (name in names(results)) {
      r <- results[[name]]
      cat(paste0("• ", name, ": MAE = ", round(r$MAE, 2),
                 ", RMSE = ", round(r$RMSE, 2),
                 ", MAPE = ", round(r$MAPE, 2), "%\n"))
    }
```

## 5.2.3 STL + ARIMA

This model decomposes the time series using STL, then forecasts the trend and seasonal components separately using ARIMA models.

```{r}
library(forecast)
    library(ggplot2)
    # STEP 1: Create full time series object (hourly frequency)
    load_ts <- ts(data_ts$total.load.actual, frequency = 24)

    # STEP 2: Split into train/test (last 168 hours = 1 week of test data)
    h <- 168
    n <- length(load_ts)
    train_ts <- head(load_ts, n - h)
    test_ts <- tail(load_ts, h)

    # STEP 3: Decompose training series using STL
    stl_decomp <- stl(train_ts, s.window = "periodic", robust = TRUE)

    # STEP 4: Extract components
    trend_component <- stl_decomp$time.series[, "trend"]
    seasonal_component <- stl_decomp$time.series[, "seasonal"]

    # STEP 5: Fit ARIMA models separately to each component
    trend_model <- auto.arima(trend_component)
    seasonal_model <- auto.arima(seasonal_component)

    # STEP 6: Forecast both components
    trend_forecast <- forecast(trend_model, h = h)
    seasonal_forecast <- forecast(seasonal_model, h = h)

    # STEP 7: Combine forecasts
    hybrid_forecast <- trend_forecast$mean + seasonal_forecast$mean

    # STEP 8: Evaluation
    rmse_hybrid <- sqrt(mean((hybrid_forecast - test_ts)^2))
    mae_hybrid <- mean(abs(hybrid_forecast - test_ts))
    mape_hybrid <- mean(abs((hybrid_forecast - test_ts) / test_ts)) * 100

    cat("STL + ARIMA RMSE:", rmse_hybrid, "\n")
    cat("STL + ARIMA MAE :", mae_hybrid, "\n")
    cat("STL + ARIMA MAPE:", mape_hybrid, "%\n")

    # STEP 9: Forecast vs Actual Plot
    time_axis <- time(test_ts)
    df_plot <- data.frame(
      Time = as.POSIXct(as.numeric(time_axis), origin = "1970-01-01"),
      Actual = as.numeric(test_ts),
      Forecast = as.numeric(hybrid_forecast)
    )

    ggplot(df_plot, aes(x = Time)) +
      geom_line(aes(y = Actual), color = "black") +
      geom_line(aes(y = Forecast), color = "red", linetype = "dashed") +
      labs(title = "STL + ARIMA Hybrid Forecast vs Actual (1 Week Ahead)",
           y = "Electricity Load (MW)", x = "Time") +
      theme_minimal()
```

```{r}
# Step 8: Extract actual values for comparison
    # Tail of original time series must match forecast horizon (h = 168)
    actual_values <- tail(load_ts, h)

    # Combine into a data frame
    forecast_vs_actual <- data.frame(
      Time = time(actual_values),
      Actual = as.numeric(actual_values),
      Forecast = as.numeric(hybrid_forecast)
    )

    # Plot
    library(ggplot2)
    ggplot(forecast_vs_actual, aes(x = Time)) +
      geom_line(aes(y = Actual), color = "black", size = 1, alpha = 0.8) +
      geom_line(aes(y = Forecast), color = "red", linetype = "dashed", size = 1) +
      labs(title = "STL + ARIMA Forecast vs Actual (Next 168 Hours)",
           y = "Electricity Load (MW)", x = "Time") +
      theme_minimal()
```

```{r}
 rmse_stl_arima <- sqrt(mean((forecast_vs_actual$Forecast - forecast_vs_actual$Actual)^2))
    mae_stl_arima <- mean(abs(forecast_vs_actual$Forecast - forecast_vs_actual$Actual))
    mape_stl_arima <- mean(abs((forecast_vs_actual$Forecast - forecast_vs_actual$Actual) / forecast_vs_actual$Actual)) * 100

    cat("STL + ARIMA RMSE:", rmse_stl_arima, "\n")
    cat("STL + ARIMA MAE:", mae_stl_arima, "\n")
    cat("STL + ARIMA MAPE:", mape_stl_arima, "%\n")
```

5.2.4 Naive Lag-1 model

```{r}
# Step 1: Read your CSV file
#data <- read.csv("C:/Users/1289549/Downloads/final_dataset_fixed (1).csv")
data <- read.csv("https://raw.githubusercontent.com/Mikhail-Broomes/Data-698/refs/heads/main/final_dataset_fixed%20(1).csv")
data$time <- as.POSIXct(data$time, format = "%m/%d/%y %H:%M", tz = "UTC")
data <- data[order(data$time), ]  # Make sure it's sorted

# Step 2: Create a train/test split
h <- 168  # Forecasting last 7 days
n <- nrow(data)
test <- data[(n - h + 1):n, ]  # last 168 rows

# Step 3: Naive lag-1 prediction
library(dplyr)
library(Metrics)
library(ggplot2)

naive_lag1_pred <- dplyr::lag(test$total.load.actual, n = 1)
actual <- test$total.load.actual[-1]
naive_lag1_pred <- naive_lag1_pred[-1]
time_series <- test$time[-1]

# Step 4: Evaluation
rmse_naive <- rmse(actual, naive_lag1_pred)
mae_naive <- mae(actual, naive_lag1_pred)
mape_naive <- mape(actual, naive_lag1_pred) * 100

cat("Naive Lag-1 RMSE:", rmse_naive, "\n")
cat("Naive Lag-1 MAE :", mae_naive, "\n")
cat("Naive Lag-1 MAPE:", mape_naive, "%\n")

# Step 5: Plot
df_naive <- data.frame(
  Time = time_series,
  Actual = actual,
  Predicted = naive_lag1_pred
)

ggplot(df_naive, aes(x = Time)) +
  geom_line(aes(y = Actual), color = "black") +
  geom_line(aes(y = Predicted), color = "blue", linetype = "dashed") +
  labs(title = "Naive Lag-1 Forecast vs Actual Load",
       y = "Electricity Load (MW)", x = "Time") +
  theme_minimal()

```

# 6.Model Evaluation 

## 6.1 Performance Summary Table

```{r}


performance_table <- data.frame(
  Model = c(
    "SARIMAX", "XGBoost", "ARIMA-ANN", "TBATS", "STL + ARIMA",
    "NNETAR (Simple)", "NNETAR (Medium)", "NNETAR (Complex)", "Naive Lag-1"
  ),
  MAE = c(2954.99, 99.08, 3076.06, 1978.49, 2223.50, 3897.11, 4118.96, 3076.06, 1145.01),
  RMSE = c(3443.95, 148.18, 3985.97, 2629.91, 2745.90, 4897.64, 5086.16, 3985.97, 1425.01),
  MAPE = c(10.81, 0.35, 12.70, 8.05, 8.48, 16.99, 17.66, 12.70, 4.45)
)

kbl(performance_table, caption = "Model Performance Summary Table", align = "lccc") %>%
  kable_classic(full_width = FALSE, html_font = "Cambria")
```

```{r}
# Build performance data frame
performance_df <- data.frame(
  Model = c(
    "SARIMAX", "XGBoost", "ARIMA-ANN", "TBATS", "STL + ARIMA",
    "NNETAR (Simple)", "NNETAR (Medium)", "NNETAR (Complex)", "Naive Lag-1"
  ),
  MAE = c(2954.99, 99.08, 3076.06, 1978.49, 2223.50, 3897.11, 4118.96, 3076.06, 1145.01),
  RMSE = c(3443.95, 148.18, 3985.97, 2629.91, 2745.90, 4897.64, 5086.16, 3985.97, 1425.01),
  MAPE = c(10.81, 0.35, 12.70, 8.05, 8.48, 16.99, 17.66, 12.70, 4.45)
)
# Reshape for plotting
performance_long <- pivot_longer(performance_df, cols = c("MAE", "RMSE", "MAPE"),
                                  names_to = "Metric", values_to = "Value")

# Bar chart by metric
ggplot(performance_long, aes(x = reorder(Model, Value), y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  facet_wrap(~Metric, scales = "free_x") +
  labs(title = "Model Performance Comparison", x = "Model", y = "Value") +
  theme_minimal()
```

In summary, this study evaluated multiple forecasting models for short-term electricity demand in Spain. Among the models tested, **XGBoost** delivered the best predictive performance across all key metrics—MAE, RMSE, and MAPE—achieving exceptional accuracy with a MAPE of only 0.35%. Its ability to integrate nonlinear feature relationships, time-based patterns, and lagged behaviors made it particularly effective.

**SARIMAX** and **ARIMA-ANN** were both effective in capturing trend and seasonality, with SARIMAX providing strong interpretability and ARIMA-ANN demonstrating hybrid modeling of residual errors. However, they were outperformed by XGBoost in forecast precision.

**TBATS** and **STL+ARIMA** handled seasonality well but showed moderate error levels. **NNETAR** models underperformed relative to others, particularly in capturing high-variance fluctuations.

The **Naive Lag-1** benchmark performed surprisingly well given its simplicity, indicating strong autocorrelation in the data.

Overall, these results support the use of tree-based ensemble methods like XGBoost for grid forecasting applications, especially where nonlinear effects and feature interactions are critical.

## 6.2 Forecast Intervals

To quantify uncertainty in the forecasts, prediction intervals were examined for the top-performing models. These intervals represent a confidence range around the forecast mean, providing a probabilistic sense of the expected variation.

-   **SARIMAX** natively supports forecast intervals via the `forecast()` function in the `forecast` package. In this study, the 80% and 95% confidence intervals indicated increasing uncertainty at longer horizons, especially around peak demand hours.

-   **XGBoost**, as a deterministic algorithm, does not provide built-in intervals. However, prediction intervals can be approximated via bootstrapping or quantile regression. In future work, methods like conformal prediction or ensembles of XGBoost models could be used to estimate interval bounds.

-   **ARIMA-ANN** also leverages `forecast()` from the `ARIMAANN` package, which includes interval estimation based on combined residual variance from ARIMA and neural components.

In the forecast visualizations, shaded regions or vertical error bars can be used to depict forecast confidence. Incorporating forecast intervals is crucial for grid operators and policymakers who must plan for both expected and extreme demand scenarios.

In summary, this study evaluated multiple forecasting models for short-term electricity demand in Spain. Among the models tested, **XGBoost** delivered the best predictive performance across all key metrics—MAE, RMSE, and MAPE—achieving exceptional accuracy with a MAPE of only 0.35%. Its ability to integrate nonlinear feature relationships, time-based patterns, and lagged behaviors made it particularly effective.

**SARIMAX** and **ARIMA-ANN** were both effective in capturing trend and seasonality, with SARIMAX providing strong interpretability and ARIMA-ANN demonstrating hybrid modeling of residual errors. However, they were outperformed by XGBoost in forecast precision.

**TBATS** and **STL+ARIMA** handled seasonality well but showed moderate error levels. **NNETAR** models underperformed relative to others, particularly in capturing high-variance fluctuations.

The **Naive Lag-1** benchmark performed surprisingly well given its simplicity, indicating strong autocorrelation in the data.

Overall, these results support the use of tree-based ensemble methods like XGBoost for grid forecasting applications, especially where nonlinear effects and feature interactions are critical.

## 6.3 Diebold-Mariano Tests

```{r}


# Ensure forecasts are defined
fc_xgb <- xgb_preds[1:168]
fc_sarimax <- predicted[1:168]
fc_arimaann <- fit_arima_ann$forecast[1:168]
fc_stl <- hybrid_forecast[1:168]

# Use test_ts as the actual values
actual <- as.numeric(test_ts)

# Run Diebold-Mariano tests
res_dm_xgb_sarimax <- dm.test(actual - fc_xgb, actual - fc_sarimax, alternative = "greater")
res_dm_xgb_arimaann <- dm.test(actual - fc_xgb, actual - fc_arimaann, alternative = "greater")
res_dm_xgb_stl <- dm.test(actual - fc_xgb, actual - fc_stl, alternative = "greater")

# Display results
cat("
Diebold-Mariano Test Results:
")
cat("XGBoost vs SARIMAX: p-value =", round(res_dm_xgb_sarimax$p.value, 4), "
")
cat("XGBoost vs ARIMA-ANN: p-value =", round(res_dm_xgb_arimaann$p.value, 4), "
")
cat("XGBoost vs STL+ARIMA: p-value =", round(res_dm_xgb_stl$p.value, 4), "
")

if (res_dm_xgb_sarimax$p.value < 0.05) cat("✅ XGBoost is significantly better than SARIMAX\n")
```





## 6.4 Residual Diagnostics

To assess model adequacy, residual diagnostics were conducted for the SARIMAX, XGBoost, ARIMA-ANN, and TBATS models. These included residual histograms, autocorrelation plots (ACF), QQ plots, and the Ljung-Box test for serial correlation.

The Ljung-Box test results yielded p-values < 0.0001 for SARIMAX, ARIMA-ANN, and XGBoost, suggesting that some autocorrelation remains in the residuals. However, this outcome is not unexpected in the context of electricity demand forecasting. Hourly energy load data are known to exhibit strong autocorrelation and seasonal structure, which even well-specified models may not fully capture — especially in short forecasting horizons or under nonlinear dynamics.

These results highlight a common trade-off in time series forecasting for energy systems: models can minimize error metrics while still leaving some structured residuals. Rather than undermining model validity, the persistence of autocorrelation indicates that further improvements could be achieved through enhanced lag structures, multi-seasonal decomposition, or ensemble-based post-model corrections.

Overall, despite minor residual autocorrelation, the models demonstrated strong predictive accuracy across evaluation metrics. In practical applications, especially for short-term grid operations, this level of performance is both acceptable and operationally useful.

```{r}
residuals_sarimax <- actual - predicted
residuals_xgb <- test_y - xgb_preds
residuals_arimaann <- test_ts - fit_arima_ann$forecast



```


```{r}
# Load necessary libraries
library(forecast)
library(ggplot2)

# Assume 'residuals_sarimax', 'residuals_xgb', 'residuals_arimaann', and 'residuals_tbats'
# are defined from previous model runs

# Function to plot diagnostics
plot_diagnostics <- function(residuals, model_name) {
  par(mfrow = c(2, 2))
  
  # Histogram
  hist(residuals, breaks = 30, main = paste("Histogram of Residuals -", model_name), xlab = "Residuals")
  
  # ACF Plot
  Acf(residuals, main = paste("ACF of Residuals -", model_name))
  
  # Ljung-Box Test
  lb_test <- Box.test(residuals, lag = 20, type = "Ljung-Box")
  print(paste("Ljung-Box p-value for", model_name, ":", round(lb_test$p.value, 5)))
  
  # QQ Plot
  qqnorm(residuals, main = paste("QQ Plot -", model_name))
  qqline(residuals)
  
  par(mfrow = c(1, 1))  # Reset layout
}

# Example usage (replace with your actual residuals)
plot_diagnostics(residuals_sarimax, "SARIMAX")
plot_diagnostics(residuals_xgb, "XGBoost")
plot_diagnostics(residuals_arimaann, "ARIMA-ANN")


```


# 7. Explainability



# 8. Discussion
The comparative analysis reveals that XGBoost consistently outperforms SARIMAX, ARIMA-ANN, and STL+ARIMA in forecasting short-term electricity demand. This superiority is evident not only in the evaluation metrics (MAE, RMSE, MAPE), but also in the Diebold-Mariano test results, where XGBoost demonstrated statistically significant improvements in predictive accuracy.
Why XGBoost Wins
Several key factors explain XGBoost’s superior performance:
Advanced Feature Handling: XGBoost is capable of efficiently modeling complex interactions between engineered features such as lag variables, rolling averages, cyclical time indicators, and weather-based exogenous inputs.


Nonlinearity and Flexibility: Unlike SARIMAX and STL-based models, which assume linear and additive structures, XGBoost can model highly nonlinear dynamics and abrupt regime shifts, making it more responsive to fluctuations in electricity consumption patterns.


Robust Generalization: XGBoost showed consistent performance across multiple forecasting horizons (daily to weekly), indicating strong generalization and stability in forecasting across different temporal contexts.


Tree-Based Resilience: Unlike neural networks (e.g., ARIMA-ANN), which are prone to overfitting and sensitivity to initialization and architecture, XGBoost leverages ensemble learning and regularization techniques that enhance robustness and interpretability.


Model-Specific Challenges
SARIMAX performed adequately and captured seasonality and exogenous trends, but struggled with nonlinear deviations and unexpected demand surges.


ARIMA-ANN attempted to capture residual nonlinearity but was constrained by the limitations of the nnet implementation in R (e.g., weight capacity, shallow architecture).


STL+ARIMA provided structured decomposition but was sensitive to long-term shifts and less responsive to real-time anomalies.


Practical Implications
The success of XGBoost in this context supports a broader trend in the energy sector—the transition from purely statistical models to machine learning-driven forecasting pipelines. As utilities face increasing volatility due to renewable integration and climate variability, adopting adaptive models like XGBoost becomes critical.
Moreover, XGBoost’s compatibility with SHAP explainability tools makes it suitable not only for operational use but also for decision support, where transparency is a regulatory and stakeholder requirement.


Interpret results, explain differences between models, discuss why XGBoost wins

# 9. Limitations & Future Work

While this study presents a comprehensive comparison of classical, machine learning, and hybrid forecasting models, several limitations remain:
Exclusion of Deep Learning Models: Although advanced models like LSTM and Transformer were considered, they were not implemented in this study due to time and computational constraints. Including these architectures could further enhance accuracy, especially for capturing long-range temporal dependencies.


Limited Interval Forecasting: The study focused on point forecasts. Prediction intervals and uncertainty quantification—particularly for machine learning models like XGBoost—were not systematically explored. Incorporating probabilistic forecasts would provide more actionable insights for risk-aware decision-making.


Restricted Dataset Range: The current analysis uses a subset of available data. Expanding the dataset to cover the full range from 2019 through 2024 would improve model robustness and better capture structural changes in energy demand, such as those caused by the COVID-19 pandemic or renewable energy growth.


No Real-Time Deployment: The models were developed and evaluated offline. Real-time forecasting and deployment in a production environment (e.g., with streaming data and periodic retraining) remain unexplored and present valuable opportunities for future work.


Future Directions
Implement deep learning architectures (e.g., LSTM, Transformer) to capture complex sequential patterns.


Explore probabilistic forecasting using quantile regression or Bayesian methods.


Integrate models into real-time systems to assess latency, retraining strategies, and operational stability.


Extend evaluation to include economic or policy-driven scenarios using external data such as tariffs or regulatory changes.


# 10. Conclusion

Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785

Hyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. https://otexts.com/fpp3/

Hong, T., Pinson, P., & Fan, S. (2014). Global energy forecasting competition 2012: Hierarchical load forecasting methods. International Journal of Forecasting, 30(2), 366–374. https://doi.org/10.1016/j.ijforecast.2013.09.001

Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 30, 4765–4774.

https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf
Hyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(3), 1–22. https://doi.org/10.18637/jss.v027.i03


